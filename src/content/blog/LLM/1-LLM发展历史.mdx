---
title: LLM发展历史
description: LLM发展历史
pubDate: 2025-10-09
toc: true
ogImage: true
category: LLM
---

LLM 英文全称为 Large Language Model，中文称之为“大语言模型”。

## LLM 和 AI

🤔 LLM 和 AI 这个词是完全等价的？或者说 AI 就是 LLM 么？

LLM 仅仅是 AI 的一个分支。

AI：Artificial Intelligence 任何能够让机器模仿人类行为的技术

- 交通程序导航
- 游戏里面的NPC

AI 下面有很多的分支：

1. 自动驾驶
2. 机器人
3. 处理语言模型
4. 处理图像模型
5. 语音识别
6. .....

机器学习（Machine Learning）就是其中之一。

机器学习：人类不直接给出决策规则，通过一套算法，让系统能够根据示例自己去学习，自己来做出决策。

**什么是深度学习**

机器学习下的一个分支，这是一种 **受到人脑神经系统的启发而发展出来的一种机器学习方法**。通过“多层神经网络”来自动从数据中学习特征和模式，尤其擅长处理图像、语音、文本等高维、复杂的数据。

> 这里仅仅是受到人脑神经的启发，不等价于复制了整个人脑。

多层：

```
输入层 --> 隐藏层1 ---> 隐藏层2 .... ---> 输出层
```

每一层包含多个神经元：

- 对输入进行加权求和
- 通过激活函数产生输出
- 输出传递给下一层

这个结构越深（隐藏层越多），模型能够学到的模式就越复杂

深度学习分支中，也不是一开始就有 Transformer 架构（模型），也是一步一步发展过来的：

1. CNN：卷积神经网络，**图像任务**为主
2. RNN / LSTM：循环神经网络，序列任务为主
3. GAN：生成对抗网络，生成类任务
4. Transformer：多模态/**语言**模型/NLP为主（现已统治 DL）

| 时间     | 模型名              | 类型         | 应用领域                       | 备注                             |
| -------- | ------------------- | ------------ | ------------------------------ | -------------------------------- |
| 1986     | MLP + 反向传播      | 通用神经网络 | 初代神经网络                   | Hinton 提出反向传播算法          |
| 1990s    | RNN                 | 循环网络     | 时间序列、语言建模             | 可记忆序列，但存在梯度消失问题   |
| 1997     | LSTM                | RNN 变体     | 更长序列建模                   | 解决 RNN 梯度问题                |
| **1998** | **LeNet-5**         | **CNN**      | 图像识别（手写数字）           | 深度学习在图像上的首次成功       |
| 2012     | **AlexNet**         | **CNN**      | 图像识别                       | ImageNet 比赛震撼业界，DL 大爆发 |
| 2014     | GAN                 | 生成模型     | 图像生成、风格迁移             | 生成对抗学习首次提出             |
| 2014     | Seq2Seq + Attention | RNN + 注意力 | 机器翻译                       | 注意力机制前身，奠基 Transformer |
| **2017** | **Transformer**     | 自注意力     | NLP，现扩展至图像/音频/多模态  | 无需循环/卷积，端到端强大架构    |
| 2018+    | BERT, GPT, T5, etc. | LLM          | 自然语言处理                   | Transformer 进入爆发期           |
| 2021+    | Diffusion Models    | 生成模型     | 图像生成（如Stable Diffusion） | GAN 之外的新方向                 |

Transformer 模型现在已经统治了深度学习领域：

| 模型            | 是否支持序列输入 | 是否并行计算   | 主要用于                     | 是否已被 Transformer 替代      |
| --------------- | ---------------- | -------------- | ---------------------------- | ------------------------------ |
| **RNN**         | ✅               | ❌（顺序依赖） | 文本、语音                   | ✅ 大多数已被 Transformer 替代 |
| **LSTM/GRU**    | ✅               | ❌             | 长序列建模                   | ✅ 被 Transformer 替代         |
| **CNN**         | ❌               | ✅             | 图像                         | 部分视觉任务仍使用             |
| **Transformer** | ✅               | ✅             | 文本、图像、音频、代码、视频 | ✅ 当前主流通用架构            |

<img
  src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-05-22-021534.png"
  alt="image-20250522101533902"
  style="zoom:50%;"
/>

理解上面几个概念之间的关系。

## LLM发展史

AI 技术两大核心应用方向

1. 计算机视觉（Computer Vision，CV）：让计算机看懂图片和视频，模仿人类的眼睛
2. 自然语言处理（Natural Language Processing，NLP）：让计算机能够理解人类语言，甚至可以进行交流

早期 AI 的研究突破基本都是和 CV 相关的，这几年 NLP 属于后来居上。

🤔思考：何为语言？信息又如何传播？

最早的语言，是以 **声音** 为媒介，通过说话进行传送的，因此使用同一种语言就显得非常重要。

不过其实，早在两千多年前，古人就研究过这个问题。古代版的普通话叫“雅言”。春秋时期，孔子的三千弟子来自五湖四海，这就必然需要孔子用一种被大家共同认可的语言来讲学。孔子用什么来讲学呢？《论语・述而第七》中记载“子所雅言，《诗》、《书》、执礼，皆雅言也。”

口头传播信息有明显的的缺点，那就是 **信息非常不易积累**，也很难传播。原始的人类开始使用结绳、刻契、图画的方法辅助记事，后来又用图形符号来简化、取代图画。当图形符号简化到一定程度，并形成与语言的特定对应的时候，早期的文字就形成了。因此，无论是最古老的象形文字、楔形文字，还是甲骨文，以及现代文字，它们的作用只有一个：**承载信息**。

<img
  src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-05-22-021436.png"
  alt="image-20250522101435700"
  style="zoom:50%;"
/>

承载信息：

1. 声音
2. 文字

没有口头话语，没有书面文字，我们就无法沟通。所以，**语言是信息的载体**。

**口头话语**和**书面文字**都是语言的重要组成部分。有了语言，就有了信息沟通的基础。不过，除了语言这个信息载体之外，我们需要在信息的通道中为语言编码和解码。

<img
  src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-05-22-022328.png"
  alt="image-20250522102328361"
  style="zoom:50%;"
/>

同理，计算机也不能直接理解人类的自然语言，因为缺少编码和解码的过程。因此，要让计算机理解我们人类的语言，就要对语言进行编码，将其转换为计算机能够读懂的形式。

NLP 的核心任务：就是让人类语言进行某种形式的编码以及解码，让计算机能够理解人类的语言。

只有理解了人类的语言，才能完成人类给它下达的任务。

## NLP演进史

分为如下 4 个阶段：

1. 起源
2. 基于规则
3. 基于统计
4. 深度学习和大数据驱动

#### 1. 起源

NLP 的起源可以追溯到阿兰・图灵在 20 世纪 50 年代提出的图灵测试。

图灵测试的基本思想：如果一个计算机程序能在自然语言对话中表现得像一个人，那么我们可以说它具有智能。

#### 2. 基于规则

在随后的数十年中，人们尝试通过基于语法和语义规则的方法来解决 NLP 问题。这也是我们人类在学习一门新的语言的时候的主要思路。

- 一门语言的规则就非常非常多，而且十分复杂，几乎没有办法基于语法规则来建模
- 语言是变化的艺术

下图是“现代中文主要语法关系示意图”

<img
  src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-05-22-023549.png"
  alt="image-20250522103549164"
  style="zoom:50%;"
/>

#### 3. 基于统计

1970 年以后，以弗雷德里克・贾里尼克为首的 IBM 科学家们采用了 **基于统计** 的方法来解决语音识别的问题，终于把**一个基于规则的问题转换成了一个数学问题**，最终使 NLP 任务的准确率有了质的提升。

至此，人们才纷纷意识到原来基于规则的方法可能是行不通的，采用统计的方法才是一条正确的道路。因此，人们基于统计定义了语言模型（Language Model）

一句话解释什么是语言模型：一种捕捉自然语言中词汇、短语和句子概率分布的**统计模型**。

| 模型 / 方法                             | 简介                                                                                                         |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Bag-of-Words（BoW）**                 | 最早期的文本表示方法之一，将文本看作词的集合，忽略语序，仅统计词频或词是否出现。广泛用于文本分类和信息检索。 |
| **N-Gram 模型**                         | 最具代表性的统计语言模型之一，利用前 N−1 个词预测下一个词，体现了语言的局部依赖性。                          |
| **Hidden Markov Model（HMM）**          | 一种生成式模型，广泛用于语音识别、词性标注等任务，结合状态转移概率和观测概率进行序列建模。                   |
| **Maximum Entropy Model（最大熵模型）** | 判别式模型，可灵活建模特征对输出的影响，常用于命名实体识别、文本分类等任务。                                 |
| **IBM 模型（1~5）**                     | IBM 提出的统计机器翻译模型，专注于词对齐、翻译概率和句子结构建模，对早期翻译系统影响深远。                   |
| **BLEU 指标（虽然不是模型，但重要）**   | 一种用于评估机器翻译结果的指标，衡量候选翻译与参考翻译的 n-gram 重合度，是机器翻译研究的重要标准。           |

这个时期，语言模型（统计模型）就已经出现了。这个时期还不能称之为大语言模型，只能叫做语言模型。

#### 4. 深度学习和大数据驱动

在统计方法被广泛应用之后，NLP 迎来了关键的技术飞跃——以 **深度学习** 和 **大数据** 为代表的新范式逐步取代了传统的统计模型，成为主流。这一阶段可以大致分为两个浪潮：

- 第一波（2013～2018）：深度学习技术开始应用于 NLP，词向量（word embedding）等方法显著提升了模型的表示能力；
- 第二波（2018～至今）：以 Transformer 为核心架构的大规模预训练模型兴起，依托海量语料数据，实现了更强的语言理解与生成能力。

深度神经网络不再只是用于特定任务的模型，而演变为通用语言表示学习工具，能够从大规模语料中自动学习语言的复杂结构与语义。

这一时期出现的**大型预训练语言模型（Large Language Models, LLMs）**，在众多 NLP 任务中的表现已达到甚至超越人类水平。它们不仅能处理文本分类、语音识别等传统任务，还可以进行高质量的自然语言生成，如对话系统、内容创作、机器翻译等。

该时期一些重要的语言模型：

| 模型                                   | 简介                                                                                         |
| -------------------------------------- | -------------------------------------------------------------------------------------------- |
| **Word2Vec（2013）**                   | Google 提出，通过上下文学习词向量，标志着词嵌入（embedding）时代的开始。                     |
| **GloVe（2014）**                      | Stanford 提出，基于共现矩阵的全局词嵌入方法。                                                |
| **ELMo（2018）**                       | 使用双向 LSTM，能根据上下文动态生成词表示，是上下文词向量的开端。                            |
| **BERT（2018）**                       | Google 提出，使用 Transformer 编码器，预训练-微调范式奠定现代 NLP 基础。                     |
| **GPT 系列（2018–至今）**              | OpenAI 提出，基于 Transformer 解码器，擅长自然语言生成，GPT-3、GPT-4 代表当今 LLM 顶尖水平。 |
| **T5（2019）**                         | Google 提出，统一 NLP 任务为“文本到文本”的框架。                                             |
| **RoBERTa、XLNet、ERNIE** 等           | 各大公司在 BERT 基础上优化推出的改进版本。                                                   |
| **ChatGPT（2022）**、**GPT-4（2023）** | 标志着 LLM 进入应用时代，表现出对话、编程、写作等通用能力。                                  |

最后有一个 NLP 发展历史表：

| 阶段               | 时间        | 方法 / 模型               | 类型                  | 主要用途                         | 是否考虑词序 / 语义     |
| ------------------ | ----------- | ------------------------- | --------------------- | -------------------------------- | ----------------------- |
| **规则阶段**       | 1950s–1970s | 语法规则、人工模板        | 人工构建规则系统      | 机器翻译、问答系统               | ✅ 语法结构，❌语义     |
| **统计阶段**       | 1970s–2010s | **Bag-of-Words (BoW)**    | 特征表示方法          | 文本分类、情感分析               | ❌                      |
|                    |             | **TF-IDF**                | 加权特征表示          | 文本检索、关键词提取             | ❌                      |
|                    |             | **N-Gram**                | 统计语言模型          | 语言建模、机器翻译               | ✅ 局部上下文，❌长依赖 |
|                    |             | **HMM**                   | 概率生成模型          | 词性标注、语音识别               | ✅                      |
|                    |             | **IBM 模型（1~5）**       | 对齐翻译模型          | 统计机器翻译                     | ✅ 局部                 |
| **深度学习阶段**   | 2013–2018   | **Word2Vec**              | 分布式词向量          | 构建语义空间、分类器输入         | ❌（静态嵌入）          |
|                    |             | **GloVe**                 | 基于矩阵的词向量      | 同上                             | ❌                      |
|                    |             | **ELMo**                  | 上下文词向量（LSTM）  | 命名实体识别、问答等             | ✅ 局部语境             |
| **大语言模型阶段** | 2018–至今   | **BERT**                  | 编码器（Transformer） | 预训练+微调，适用于多数 NLP 任务 | ✅ 深层语义             |
|                    |             | **GPT 系列（GPT-2/3/4）** | 解码器（Transformer） | 文本生成、对话系统               | ✅ 强上下文与推理能力   |
|                    |             | **T5**                    | 编码器-解码器结构     | 多任务文本生成                   | ✅                      |
|                    |             | **ChatGPT**               | 应用层 LLM            | 对话、创作、问答                 | ✅ 多轮对话上下文       |

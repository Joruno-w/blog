---
title: 提示词常见技巧
description: 提示词常见技巧
pubDate: 2025-10-09
toc: true
ogImage: true
category: LLM
---

- 少样本学习
- 逐步思考

## 少样本学习

少样本学习（few-shot learning），也称之为少样本提示，这是一种范式，在这种范式中，模型通过提示词获得你希望模型执行的任务示例。

给大模型提供2～5条 任务 - 结果对（上下文）

接着给大模型一个新的任务，让大模型模仿前面的例子给出答案。

少样本学习的优势：

1. 快速适应新任务
2. 灵活应对多种格式
3. 提高准确性/稳定性
4. **更可控的输出结构**

示例：

```
例子：
评论：这个产品真不错，用得很顺手！ → 情感：正面
评论：完全失望，质量太差了。 → 情感：负面
评论：快递到了。 → 情感：中性

评论：外观还行，性能一般般。 → 情感：
```

```
例子：
句子：马云是阿里巴巴的创始人。
实体：[马云: 人名], [阿里巴巴: 组织]

句子：乔布斯出生在旧金山。
实体：[乔布斯: 人名], [旧金山: 地点]

句子：李雷明天去北京出差。
实体：
```

```
例子：
姓名：张三，年龄：28，职业：工程师
→ {"name": "张三", "age": 28, "job": "工程师"}

姓名：李四，年龄：35，职业：设计师
→ {"name": "李四", "age": 35, "job": "设计师"}

姓名：王五，年龄：40，职业：产品经理
→
```

## 逐步思考

> 369 \* 1235 是多少？

早期的大模型是无法回答这个问题的，给出的答案千奇百怪。

🤔为什么大模型不能解决这个问题？

因为大模型是从左到右，一个词元一个词元的去预测，每一个数字都是预测出来的，而非计算出来的。

逐步思考：不是让大模型直接给出答案，而是通过提示词让大模型进行**中间推理**。

如何做：在提示词之后添加“Let's think step by step（让我们一步一步思考）”

这种策略被称之为 **零样本思维链策略（zero-shot-CoT strategy）**，最早由 Kojima 等人于 2022 年发表在一篇名为“Large Language Models Are Zero-Shot Reasoners”的科学论文中。

- 零样本：不提供任何的样本
- 思维链：让大模型一步一步进行推理，然后得到答案

示例：

```
Q: 小明有 3 个苹果，小红给了他 2 个苹果，然后他吃掉了 1 个。问他现在有几个苹果？
请一步步思考。
```

```
Q: 如果所有猫都会爬树，汤姆是猫，那么汤姆会爬树吗？
请一步步思考。
```

```
Q: 今天是星期三，那么 10 天后是星期几？
请一步步思考。
```

```
Q: 小王有 4 支红笔和 5 支蓝笔，他给了朋友 2 支蓝笔和 1 支红笔。现在他还有几支笔？
请一步步思考。
```

```
Q: 如果下雨了，地会湿；如果地湿了，孩子们就不能出去玩。现在下雨了。那么孩子们能出去玩吗？
请一步步思考。
```

总结一下：所谓逐步思考，就是指在零样本的情况下，让大模型一步一步思考，最终得到答案。

常见的提示词：

1. 请一步一步思考
2. 请解释你的推理过程
3. 请详细说明你是如何得到这个答案的

## N-shot Learning

少样本学习：few-shot learning 给一些少量的样本，大模型会基于你所给的样本，给出问题的答案

零样本学习：zero-shot Learning 完全不给样本，让大模型自己一步一步思考，得出答案

One-shot Learning：给一个样本，然后询问大模型问题

```
Prompt:
例子：
Input: "Good morning" → Output: "早上好"

请翻译：
Input: "Thank you" → Output:
```

---

-EOF-

---
title: 本地部署大模型
description: 本地部署大模型
pubDate: 2025-10-09
toc: true
ogImage: true
category: LLM
---

要使用大模型，可以通过两种方式：

1. 远程 API 调用
2. **本地部署**

**远程 API 调用**

模型由特定组织开发以及托管，例如OpenAI 的 GPT-4 和 Anthropic 的 Claude，用户通过 API 接口来访问。

<img
  src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-05-25-134913.png"
  alt="image-20250525214913221"
  style="zoom:50%;"
/>

优点：

1. 功能强大
2. 用户无须拥有强大的 GPU 也能够使用功能强大的 LLM

缺点：

1. 费用
2. 隐私

**本地部署**

通过一些工具来做本地部署，部署到用户自己的设备上面。

一般来讲，部署的都是**开源模型**，例如 Meta 的 Llama、微软的 Phi 、阿里的千问、以及 Deepseek 等模型

<img
  src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-05-25-140953.png"
  alt="image-20250525220953139"
  style="zoom:50%;"
/>

学习阶段选择本地部署，部署一个功能不太强的模型。

**安装 Ollama**

[Ollama](https://ollama.com/) 是一个开源的大语言模型**运行平台**，提供简洁易用的 **命令行工具** 和 **REST API**，帮助用户在本地轻松下载、部署和运行各种开源的 LLM（大语言模型）

Ollama 是 Meta 公司推出的模型平台，Meta 公司还推出了自家的大语言模型 Llama

Ollama 特点：

1. 开源免费
2. 即装即用
3. 支持多种模型：Llama、deepseek、qwen...
4. 本地运行
5. 灵活扩展
6. **提供 REST API**：本地会启动一个 Web 服务，监听 11434 端口

常用命令：

1. ollama list：查看已下载模型
2. ollama show 「模型名称」：显示模型信息
3. ollama pull 「模型名称」：拉取模型
4. ollama run 「模型名称」：拉取并且运行模型
   1. 存在：直接运行模型
   2. 不存在：先拉取下来，然后运行
5. ollama run 「模型名称」 --verbose：查看每次对话后模型执行的效率细节

默认启动模型后，支持的是长期对话，如果是一次性对话，那么可以：

```bash
ollama run 「模型名称」 "提示词"
```

也支持多行提示词对话。在启动模型后，将多行对话放入三个双引号里面就能够输入多行提示词。

```bash
ollama run llama3.2
>"""
>提示词内容
>提示词内容
>提示词内容
>"""
```

**官方模型库**

在 [官网模型库](https://ollama.com/search) 中，Ollama 提供了丰富的模型选型，这些模型大致可以分为两大类：

1. 语言处理类大模型
2. 图像处理类大模型

**语言处理类大模型（LLMs）**

这类模型专注于自然语言生成与理解，常用于文本生成、问答、代码编写、翻译、总结等任务。Ollama 支持的主要语言模型包括：

| 模型名称        | 来源机构        | 规模（参数量） | 主要特点                                   |
| --------------- | --------------- | -------------- | ------------------------------------------ |
| **LLaMA 2 / 3** | Meta            | 7B / 13B / 70B | 通用语言理解与生成，支持中文、对话能力强   |
| **Mistral**     | Mistral AI      | 7B             | 小而强的模型，推理速度快，多语言支持       |
| **Gemma**       | Google DeepMind | 2B / 7B        | 开源许可宽松，轻量且效率高，适合嵌入式应用 |
| **Phi**         | Microsoft       | 2.7B           | 适合教育和低算力环境，训练数据精细设计     |
| **Code LLaMA**  | Meta            | 7B+            | 专注于编程任务，如代码补全、代码解释等     |
| **Neural Chat** | Intel           | 多种参数量     | 聊天风格自然，适合交互式对话               |

此外，Ollama 也支持部分**微调版本模型**（如 `llama2-chinese`, `mistral-instruct`, `orca-mini` 等），可以根据任务需求选择指令微调或对话优化模型。

这里的参数量以 B 为单位，B 来自于英语单词 billion（十亿），因此例如参数量写的是 7B，则意味着参数量为 70亿。

**图像处理类大模型（Vision / Multimodal Models）**

Ollama 近期开始逐步支持 **图像 + 文本联合建模** 的多模态模型（Multimodal），如：

| 模型名称     | 来源机构   | 模态类型               | 说明                               |
| ------------ | ---------- | ---------------------- | ---------------------------------- |
| **llava**    | LLaVA 项目 | 图文联合（image+text） | 支持图像描述、图像问答、OCR 等任务 |
| **bakllava** | 社区开源   | 多模态                 | 类似 LLaVA，支持多图问答           |
| **clip**     | OpenAI     | 图像 + 文本            | 图文相似度匹配，适合搜索、推荐系统 |

这类模型适合构建如：**AI 看图说话、图片问答助手、图像内容搜索**等系统。由于涉及图像输入，目前仍需配合本地 API 接口使用，CLI 中支持有限。

---

-EOF-

---
title: å­—ç¬¦ä¸²è§£æå™¨
description: å­—ç¬¦ä¸²è§£æå™¨
pubDate: 2025-09-28
toc: true
ogImage: true
category: Langchain
---

**ä¸ºä»€ä¹ˆéœ€è¦è§£æå™¨ï¼Ÿ**

é»˜è®¤æƒ…å†µä¸‹ï¼ŒLangChain ä¸­è°ƒç”¨ LLM è¿”å›çš„å…¶å®ä¸æ˜¯ä¸€ä¸ªç®€å•çš„å­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ä¸€ä¸ªåŒ…å«å¤§é‡å…ƒä¿¡æ¯çš„å¯¹è±¡ã€‚

è¯¾å ‚ç¤ºä¾‹

```js
import { ChatOllama } from '@langchain/ollama'
import { HumanMessage } from '@langchain/core/messages'

const chatModel = new ChatOllama({
  model: 'llama3',
  temperature: 0.7,
})

const response = await chatModel.invoke([new HumanMessage('Tell me a joke')])

console.log('Response:', response)
```

```
Response: AIMessage {
  "content": "Here's one:\n\nWhy couldn't the bicycle stand up by itself?\n\n(wait for it...)\n\nBecause it was two-tired!\n\nHope that made you smile! Do you want to hear another one?",
  "additional_kwargs": {},
  "response_metadata": {
    "model": "llama3",
    "created_at": "2025-06-20T01:35:00.765212Z",
    "done_reason": "stop",
    "done": true,
    "total_duration": 2370986375,
    "load_duration": 56247875,
    "prompt_eval_count": 14,
    "prompt_eval_duration": 1146345791,
    "eval_count": 40,
    "eval_duration": 1167879375
  },
  "tool_calls": [],
  "invalid_tool_calls": [],
  "usage_metadata": {
    "input_tokens": 14,
    "output_tokens": 40,
    "total_tokens": 54
  }
}
```

æœ‰ç”¨çš„å…ƒæ•°æ®ï¼Œä¾‹å¦‚ï¼š

- ä½¿ç”¨çš„æ¨¡å‹åç§°å’Œç”Ÿæˆæ—¶é—´ï¼ˆresponse_metadataï¼‰
- token ä½¿ç”¨æƒ…å†µï¼ˆusage_metadataï¼‰
- å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆtool_callsï¼‰

è¿™å¯¹äºå¼€å‘è€…æ¥è¯´å¾ˆæœ‰ä»·å€¼ï¼Œä½†**å¯¹ç»ˆç«¯ç”¨æˆ·æ¥è¯´ï¼Œè¿™äº›æ˜¯â€œå™ªéŸ³â€**ã€‚

å½“ç„¶æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨ä»è¿”å›å¯¹è±¡ä¸­æå– `response.content`ï¼Œä½†è¿™åœ¨å¤šä¸ªæ¨¡å‹ã€å¤šä¸ªæ¡†æ¶ä¹‹é—´åˆ‡æ¢æ—¶ä¼šå¾ˆç¹çï¼š

- ä¸åŒæ¨¡å‹è¿”å›çš„æ ¼å¼ä¸å®Œå…¨ä¸€è‡´
- æœ‰äº›æ¨¡å‹ç”šè‡³ç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼ˆå¦‚åŸºç¡€æ¨¡å‹ï¼‰
- ä½ æ¯æ¬¡éƒ½è¦å†™ `response.content`ï¼Œå†™å¤šäº†å¾ˆå®¹æ˜“å‡ºé”™

è¿™å°±æ˜¯è§£æå™¨ç™»åœºçš„æ„ä¹‰ã€‚

**è§£æå™¨çš„ä½œç”¨**

æ‹¿åˆ°æ¨¡å‹çš„è¾“å‡ºåï¼Œåˆ©ç”¨è§£æå™¨å¯ä»¥å¸®åŠ©æˆ‘ä»¬å°† LLM åŸå§‹è¾“å‡ºè½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®ï¼Œå¸¸ç”¨äºï¼š

- æå– JSONã€åˆ—è¡¨ã€å¯¹è±¡ã€ä»£ç å—
- æ•°æ®æ¸…æ´—ä¸æ ¼å¼åŒ–
- é…åˆ Tool / Agent æå–æŒ‡ä»¤å‚æ•°

**è§£æå™¨ç±»å‹**

1. å­—ç¬¦ä¸²è§£æå™¨
2. ç»“æ„åŒ–è¾“å‡ºè§£æå™¨
3. åˆ—è¡¨è¾“å‡ºè§£æå™¨
4. ....

**å­—ç¬¦ä¸²è§£æå™¨**

åœ¨å®é™…å¼€å‘ä¸­ï¼Œæˆ‘ä»¬å¾ˆå¤šæ—¶å€™**åªå…³å¿ƒæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹**ï¼Œè€Œä¸æ˜¯ä¸€å¤§ä¸²åŒ…å«å…ƒä¿¡æ¯çš„å¤æ‚å¯¹è±¡ã€‚

æ­¤æ—¶å°±å¯ä»¥ä½¿ç”¨ `StringOutputParser` â€”â€” å®ƒæ˜¯æœ€åŸºç¡€ã€æœ€å¸¸ç”¨çš„è¾“å‡ºè§£æå™¨ã€‚

```js
import { ChatOllama } from '@langchain/ollama'
import { HumanMessage } from '@langchain/core/messages'
// å¼•å…¥è¾“å‡ºè§£æå™¨
import { StringOutputParser } from '@langchain/core/output_parsers'

// åˆ›å»ºèŠå¤©æ¨¡å‹å®ä¾‹
const chatModel = new ChatOllama({
  model: 'llama3',
  temperature: 0.7,
})

// åˆ›å»ºè§£æå™¨ï¼Œå¹¶é€šè¿‡ pipe() è¿æ¥æ¨¡å‹
const parser = new StringOutputParser()
const chain = chatModel.pipe(parser)

// å‘èµ·è°ƒç”¨ï¼Œç›´æ¥å¾—åˆ°çº¯æ–‡æœ¬ç»“æœ
const response = await chain.invoke([new HumanMessage('ç”¨ä¸­æ–‡ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯')])

console.log('ğŸ¤– å“åº”æ–‡æœ¬ï¼š', response)
```

è¿™æ˜¯æœ€ç®€å•çš„ Parserï¼Œå¦‚æœä¸ä½¿ç”¨ `StringOutputParser`ï¼Œä½ å¾—åˆ°çš„æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„å¯¹è±¡ï¼š

```js
AIMessage {
  content: "...",
  usage_metadata: {...},
  response_metadata: {...},
  ...
}
```

éœ€è¦æ‰‹åŠ¨æå– `.content` å­—æ®µæ‰èƒ½ä½¿ç”¨ã€‚ä½†é€šè¿‡ `StringOutputParser`ï¼Œèƒ½å¤Ÿ**è‡ªåŠ¨ä¸ºå®Œæˆè§£æå’Œæå–**ï¼Œçœå»äº†ç¹ççš„æ­¥éª¤ã€‚

æµå¼ç¤ºä¾‹ï¼š

```js
import { ChatOllama } from '@langchain/ollama'
import { HumanMessage } from '@langchain/core/messages'
import { StringOutputParser } from '@langchain/core/output_parsers'

// åˆ›å»ºæ”¯æŒæµå¼å“åº”çš„æ¨¡å‹å®ä¾‹
const chatModel = new ChatOllama({
  model: 'llama3',
  temperature: 0.7,
})

// åˆ›å»ºè¾“å‡ºè§£æå™¨ï¼Œå¹¶æ„å»º chain
const parser = new StringOutputParser()
const chain = chatModel.pipe(parser)

// å‘èµ·æµå¼è°ƒç”¨
const stream = await chain.stream([new HumanMessage('ç”¨ä¸­æ–‡ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯')])

// é€å—è¾“å‡ºå“åº”å†…å®¹
for await (const chunk of stream) {
  process.stdout.write(chunk)
}
```

---
title: 模型接口
description: 模型接口
pubDate: 2025-09-28
toc: true
ogImage: true
category: Langchain
---

LangChain 支持两类主流语言模型：

1. 文本补全模型
2. 对话模型

## 文本补全模型

Text Completion Models

这类模型以**一段纯文本**作为**输入**，输出结果是**一段连续生成的文字**（这里的输出文本其实就是对前面输入文本的一个补全），**不区分说话角色**，也**不会自动记录上下文**。它们通常用于单轮任务，如：段落续写、摘要生成、内容润色等。
代表模型包括早期的 GPT-3（如 `text-davinci-003`）或本地推理环境下的 LLaMA2 系列。

在上一节课中，我们动手实现了一个 LangChain 快速上手示例：

```js
import { Ollama } from '@langchain/ollama'

const model = new Ollama({
  model: 'llama3',
})

const res = await model.invoke('你是谁？')

console.log(res)
```

这个案例中的 `Ollama` 实际封装的是一类基础补全模型（Text Completion Model）。这类模型具备以下典型特征：

1. 输入格式：接收的是 **普通文本字符串**，而 **不是对话消息数组**
2. 输出格式：返回的同样是 **一段纯文本**，**不含结构化角色信息**
3. 适用场景：更适合非对话类任务，例如：摘要提取、句子改写、文本分类、代码生成等
4. 调用方式：使用 `invoke(text)` 方法，输入为字符串
5. 与聊天模型区别：不支持 `HumanMessage` / `AIMessage` 角色区分，也不保留上下文

## 对话模型

Chat Completion Models

对话模型在能力上更进一步，它们支持**多轮上下文**和**角色区分**，输入格式为一组带有身份标签的消息**数组**（例如 `system`、`user`、`assistant`），模型能够根据对话历史生成更加自然且连续的回复。
代表性模型有 GPT-4、Claude 3、Gemini、Mistral Chat 等，常用于聊天机器人、Agent 推理流程等复杂交互场景。

接下来我们来看看一个对话模型的实战示例。在开始之前，先安装一个必要的依赖包：

```js
pnpm add @langchain/core
```

在这里，我们也顺便聊一聊 LangChain 的包结构。整个 LangChain 生态系统由多个模块化包组成，各自承担不同职责，协同构建完整的智能应用开发框架：

1. **`@langchain/core`**
   LangChain 的“内核”，提供**最基础的抽象能力**，比如 `Runnable`（可执行单元）、`Prompt`（提示词模板）、`OutputParser`（输出解析器）等。同时还包含 LangChain Expression Language，用于编排复杂的链式逻辑。
2. **`@langchain/community`**
   社区维护的扩展包，收录了大量第三方集成，例如各类向量数据库、模型 API、文档加载器等。可以理解为 LangChain 的“插件市场”。
3. **`langchain`**
   是在核心能力基础上的**高阶封装**，内置了常见的链式组件（Chains）、智能体模块（Agents）、工具调度（Tools）等，是大多数 LangChain 应用的默认入口包。

```js
import { ChatOllama } from '@langchain/ollama'
import {
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
  ChatPromptTemplate,
} from '@langchain/core/prompts'

const model = new ChatOllama({
  model: 'llama3',
  temperature: 0.7,
  stream: true,
})

// 创建多角色的提示词模板

// 1. 创建系统提示词
const sysPrompt = SystemMessagePromptTemplate.fromTemplate(
  '你是一个翻译助理 ,请将用户输入的内容由{input_language}直接翻译为{output_language}.'
)

// 2. 创建用户提示词
const humanPrompt = HumanMessagePromptTemplate.fromTemplate('{text}')

// 3. 合成提示词
const chatPrompt = ChatPromptTemplate.fromMessages([sysPrompt, humanPrompt])

// 4. 填充变量
const messages = await chatPrompt.formatMessages({
  input_language: '中文',
  output_language: '英语',
  text: '今天天气真好，我们去游泳吧',
})

const res = await model.stream(messages)

for await (const chunk of res) {
  process.stdout.write(chunk.content)
}
```

**简单类比：**

- 文本补全模型：像是让 AI 接着你写的作文续写一段内容；
- 对话模型：更像是你在和一个“懂上下文”的 AI 助手对话，支持来回沟通。

LangChain 中的两种本地模型调用方式 `Ollama` 和 `ChatOllama` 对比如下：

| 对比维度        | `Ollama`（文本补全模型）                           | `ChatOllama`（对话模型）                                  |
| --------------- | -------------------------------------------------- | --------------------------------------------------------- |
| 所属包          | 同为 `@langchain/ollama`                           | 同为 `@langchain/ollama`                                  |
| 继承体系        | 基于 `LLM` 抽象类                                  | 基于 `BaseChatModel` 抽象类                               |
| 输入类型        | 接收 **纯文本字符串** 作为 prompt                  | 接收 **消息对象数组**（含 `system`、`user`、`assistant`） |
| Prompt 模板支持 | 支持文本模板（单角色）                             | 支持多角色模板（含系统提示 + 用户消息）                   |
| 系统提示能力    | 不支持 `SystemMessage`，需手动写入 prompt 中       | 原生支持系统消息（如使用 `SystemMessagePromptTemplate`）  |
| 上下文记忆      | 不具备上下文感知能力，每次调用是独立的             | 支持对话上下文，可构建多轮逻辑                            |
| 使用方式        | 适合执行 `model.invoke("文本")` 类型的**单轮任务** | 可通过 `chatModel.invoke(messages)` 处理**多轮对话**      |
| 推荐场景        | 适用于摘要、问答、标题生成等**单轮文本生成任务**   | 适用于翻译、聊天机器人、角色扮演等**多轮交互任务**        |

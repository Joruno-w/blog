---
title: Runnableæ¥å£
description: Runnableæ¥å£
pubDate: 2025-09-28
toc: true
ogImage: true
category: Langchain
---

åœ¨ LCEL ä¸­ï¼Œå‡ ä¹æ‰€æœ‰çš„æ¨¡å—ï¼š

- æç¤ºè¯æ¨¡æ¿
- æ¨¡å‹
- è§£æå™¨

éƒ½æ˜¯å®ç°äº† Runnable æ¥å£çš„ï¼Œå¯ä»¥å°†è¿™äº›æ¨¡å—ç§°ä¹‹ä¸º Runnable ç±»å‹ã€‚è¿™ç§ç±»å‹çš„æ¨¡å—å¯ä»¥å¿«é€Ÿæ’å…¥åˆ°é“¾æ¡é‡Œé¢ã€‚

**RunnableLambda**

`RunnableLambda` æ˜¯ LangChain.js æä¾›çš„ä¸€ç§è½»é‡çº§å·¥å…·ï¼Œå®ƒèƒ½æŠŠ **æ™®é€šå‡½æ•°** å°è£…æˆç¬¦åˆ `Runnable` æ¥å£è§„èŒƒçš„å®ä¾‹ï¼Œä»è€Œè®©è¯¥å‡½æ•°èƒ½å¤Ÿæ— ç¼å‚ä¸åˆ° LCEL çš„é“¾å¼è°ƒç”¨ä¸æµå¼å¤„ç†æµç¨‹ä¸­ã€‚

1. å¿«é€Ÿä¸Šæ‰‹æ¡ˆä¾‹

   ```js
   import { RunnableLambda } from '@langchain/core/runnables'

   const fn = (text) => {
     return text.toUpperCase()
   }

   const runnableFn = RunnableLambda.from(fn)

   const res = await runnableFn.invoke('hello')

   console.log(res)
   ```

2. å…³é”®è¯é«˜äº®æ’ä»¶

   `é—­åŒ…` --> `*&*é—­åŒ…*&*`

   ```js
   import { ChatOllama } from '@langchain/ollama'
   import { PromptTemplate } from '@langchain/core/prompts'
   import { StringOutputParser } from '@langchain/core/output_parsers'
   import { RunnableLambda } from '@langchain/core/runnables'

   // 1. åˆ›å»ºä¸€ä¸ªæç¤ºè¯æ¨¡æ¿
   const pt = PromptTemplate.fromTemplate('è¯·ä½¿ç”¨ä¸­æ–‡è§£é‡Šä¸‹é¢çš„æ¦‚å¿µï¼š{topic}')

   // 2. åˆ›å»ºæ¨¡å‹
   const model = new ChatOllama({
     model: 'llama3',
     temperature: 0.7,
   })

   // 3. è§£æå™¨
   const parser = new StringOutputParser()

   // 4. åˆ›å»ºä¸€ä¸ªé“¾æ¡
   let chain = pt.pipe(model).pipe(parser)

   // 5. åˆ›å»ºä¸€ä¸ªç®€å•çš„æ’ä»¶
   const fn = (text) => text.replace(/é—­åŒ…/g, '*&*é—­åŒ…*&*')
   const highlight = RunnableLambda.from(fn)

   chain = chain.pipe(highlight)

   const res = await chain.invoke({
     topic: 'é—­åŒ…',
   })

   console.log(res)
   ```

æ›´å¤šåœºæ™¯ï¼š

- æ’å…¥æ—¥å¿—æ”¶é›†æ¨¡å—
- æ’å…¥ç¿»è¯‘æ¨¡å—ï¼ˆè°ƒç”¨å¤–éƒ¨ APIï¼‰
- æ’å…¥å¼€å…³ï¼šåˆ¤æ–­æŸä¸ªæ¡ä»¶æ˜¯å¦ä¸­æ–­æ‰§è¡Œ
- æ ¼å¼è°ƒæ•´ã€ç»“æ„æ¸…æ´—ã€æ•æ„Ÿè¯è¿‡æ»¤

`pipe()` æ–¹æ³•å¯æ¥å—çš„ä¸‰ç§ç±»å‹ï¼š

1. Runnable å®ä¾‹ï¼šè¿™æ˜¯**æœ€å¸¸ç”¨çš„ï¼Œä¹Ÿæ˜¯æœ€æ¨èçš„**

2. æ™®é€šå‡½æ•°ï¼šLCEL å†…éƒ¨ä¼šè‡ªåŠ¨ç”¨ `RunnableLambda.from(fn)` åŒ…è£…æˆä¸€ä¸ª `Runnable`ã€‚

3. å¯¹è±¡ï¼šå°†ä¸Šæ¸¸çš„è¾“å…¥ï¼ˆæˆ–ç»“æœï¼‰åˆ†åˆ«ä¼ ç»™å¤šä¸ª runnableï¼Œç„¶åè¿”å›ä¸€ä¸ªå¯¹è±¡ã€‚

   ```js
   import { ChatOllama } from '@langchain/ollama'
   import { PromptTemplate } from '@langchain/core/prompts'
   import { StringOutputParser } from '@langchain/core/output_parsers'
   import { RunnableLambda } from '@langchain/core/runnables'

   // åˆ›å»ºæ¨¡å‹
   const model = new ChatOllama({
     model: 'llama3',
     temperature: 0.7,
   })

   // è§£æå™¨
   const parser = new StringOutputParser()

   // åˆ›å»ºä¸¤ä¸ªå­é“¾
   const chain1 = PromptTemplate.fromTemplate(
     'è¯·ç”¨ä¸­æ–‡ç”¨ 2-3 å¥æ¦‚æ‹¬ä»¥ä¸‹ä¸»é¢˜çš„æ ¸å¿ƒå«ä¹‰ï¼š{input}'
   )
     .pipe(model)
     .pipe(parser)

   const chain2 = PromptTemplate.fromTemplate(
     'è¯·ç”¨ä¸­æ–‡ä»ä»¥ä¸‹ä¸»é¢˜ä¸­æå– 5 ä¸ªå…³é”®è¯ï¼Œä»¥é€—å·åˆ†éš”ï¼š{input}'
   )
     .pipe(model)
     .pipe(parser)

   let chain = RunnableLambda.from((x) => x)

   chain = chain.pipe({
     summary: chain1,
     keywords: chain2,
   })

   const res = await chain.invoke({
     input: 'é—­åŒ…',
   })
   console.log(res)
   ```

**RunnableMap**

`RunnableMap`ï¼ˆä¹Ÿå« `RunnableParallel`ï¼‰ï¼Œå®ƒå¯ä»¥è®©å¤šä¸ªé“¾æ¡ **å¹¶å‘æ‰§è¡Œ**ï¼Œå¹¶è¿”å›ä¸€ä¸ªç»“æ„åŒ–çš„å¯¹è±¡ç»“æœã€‚

`RunnableMap.from({ ... })` ä¼šå¹¶å‘æ‰§è¡Œå¤šä¸ªå­é“¾ï¼Œå¹¶å°†å®ƒä»¬çš„ç»“æœç»„åˆæˆä¸€ä¸ªå¯¹è±¡è¿”å›ã€‚

```js
import { ChatOllama } from '@langchain/ollama'
import { PromptTemplate } from '@langchain/core/prompts'
import { StringOutputParser } from '@langchain/core/output_parsers'
import { RunnableMap } from '@langchain/core/runnables'

// åˆ›å»ºæ¨¡å‹
const model = new ChatOllama({
  model: 'llama3',
  temperature: 0.7,
})

// è§£æå™¨
const parser = new StringOutputParser()

const chain1 = PromptTemplate.fromTemplate('ç”¨ä¸­æ–‡è®²ä¸€ä¸ªå…³äº {topic} çš„ç¬‘è¯')
  .pipe(model)
  .pipe(parser)
const chain2 = PromptTemplate.fromTemplate('ç”¨ä¸­æ–‡å†™ä¸€é¦–å…³äº {topic} çš„ä¸¤è¡Œè¯—')
  .pipe(model)
  .pipe(parser)

const chain = RunnableMap.from({
  joke: chain1,
  poem: chain2,
})

const res = await chain.invoke({
  topic: 'å°ç‹—',
})
console.log(res)
```

**RunnableSequence**

`.pipe()` æ˜¯é€ä¸ªæ‹¼æ¥ï¼Œ`RunnableSequence.from([...])` åˆ™æ˜¯æ˜¾å¼å£°æ˜æµç¨‹ç»“æ„ï¼Œå°†å¤šä¸ªæ­¥éª¤å†™æˆæ•°ç»„æ›´æ¸…æ™°ã€‚

è¯¾å ‚ç»ƒä¹ ï¼šå¿«é€Ÿä¸Šæ‰‹ç¤ºä¾‹

```js
import { ChatOllama } from '@langchain/ollama'
import { PromptTemplate } from '@langchain/core/prompts'
import { StringOutputParser } from '@langchain/core/output_parsers'
import { RunnableLambda, RunnableSequence } from '@langchain/core/runnables'

// 1. åˆ›å»º Prompt æ¨¡æ¿
const pt = PromptTemplate.fromTemplate('è¯·ä½¿ç”¨ä¸­æ–‡è§£é‡Šä»¥ä¸‹æ¦‚å¿µï¼š{topic}')

// 2. åˆ›å»ºæœ¬åœ°æ¨¡å‹
const model = new ChatOllama({ model: 'llama3', temperature: 0.7 })

// 3. åˆ›å»ºå­—ç¬¦ä¸²è§£æå™¨
const parser = new StringOutputParser()

// 4. æ’ä»¶
const fn = (text) => {
  return text.replace(/é—­åŒ…/g, '*&*é—­åŒ…*&*')
}
const highlight = RunnableLambda.from(fn)

const chain = RunnableSequence.from([pt, model, parser, highlight])

const res = await chain.invoke({
  topic: 'é—­åŒ…',
})

console.log(res)
```

**RunnablePassthrough**

å°†è¾“å…¥åŸæ ·ä¼ ç»™è¾“å‡ºï¼Œä¸­é—´ä¸åšä»»ä½•å¤„ç†ã€‚

```js
import { RunnablePassthrough } from '@langchain/core/runnables'

const passthrough = new RunnablePassthrough()

// ç›¸å½“äºè¾“å…¥ä»€ä¹ˆï¼Œè¾“å‡ºå°±æ˜¯ä»€ä¹ˆ
const result = await passthrough.invoke('Hello, LangChain!')
console.log(result) // è¾“å‡ºï¼šHello, LangChain!
```

ğŸ¤” è¿™æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ

å®ä¾‹åŒ– RunnablePassthrough çš„æ—¶å€™ï¼Œæ¥æ”¶ä¸€ä¸ªé…ç½®å¯¹è±¡ï¼Œè¯¥å¯¹è±¡ä¸­å¯ä»¥é…ç½® func å‰¯ä½œç”¨å‡½æ•°ï¼Œç”¨äºå¯¹è¾“å…¥åšä¸€äº›å‰¯ä½œç”¨å¤„ç†ï¼Œä¾‹å¦‚è®°å½•æ—¥å¿—ã€å†™å…¥æ•°æ®åº“ã€åšåŸ‹ç‚¹ç­‰ã€‚

```js
import { RunnablePassthrough } from '@langchain/core/runnables'

const logger = new RunnablePassthrough({
  // ä¸€ä¸ªå‰¯ä½œç”¨å‡½æ•°
  func: async (input) => {
    console.log('æ”¶åˆ°è¾“å…¥ï¼š', input)
    // å¯¹è¾“å…¥åšä¸€äº›å‰¯ä½œç”¨å¤„ç†ï¼Œä¾‹å¦‚è®°å½•æ—¥å¿—ã€å†™å…¥æ•°æ®åº“ã€åšåŸ‹ç‚¹ç­‰
  },
})

const res = await logger.invoke('LangChain')
console.log(res)
```

æœ‰æ—¶å¸Œæœ›ä¸ºä¼ å…¥çš„å¯¹è±¡è¡¥å……å­—æ®µï¼ˆå¦‚æ—¶é—´æˆ³ã€ç”¨æˆ·ä¿¡æ¯ã€API ç»“æœç­‰ï¼‰ï¼Œè¿™æ—¶å¯ä»¥ä½¿ç”¨ `.assign()`ã€‚ å®ƒçš„ä½œç”¨æ˜¯â€œç»™ä¸Šä¸‹æ–‡æ·»åŠ å­—æ®µâ€ï¼Œå°±åƒåœ¨ä¸­é—´æ’å…¥ä¸€æ­¥ Object.assignï¼š

```js
import { RunnablePassthrough } from '@langchain/core/runnables'

const injector = RunnablePassthrough.assign({
  timestamp: async () => new Date().toISOString(),
  meta: async () => ({
    region: 'us-east',
    requestId: 'req-456',
  }),
})

const result = await injector.invoke({ query: 'Vue æ˜¯ä»€ä¹ˆï¼Ÿ' })

console.log(result)
/*
  {
    query: 'Vue æ˜¯ä»€ä¹ˆï¼Ÿ',
    timestamp: '2025-08-11T08:07:35.358Z',
    meta: { region: 'us-east', requestId: 'req-456' }
  }
*/
```

å¯ä»¥ç”¨å®ƒæ¥ï¼š

- æ’å…¥æ—¶é—´æˆ³ã€ç”¨æˆ· IDã€è¯·æ±‚ ID
- æ³¨å…¥å·¥å…·ç»“æœï¼šæ‘˜è¦ã€æ ‡ç­¾ã€ç¿»è¯‘ã€æƒ…ç»ªåˆ†æ
- ä¸º Prompt æ·»åŠ é¢å¤–å­—æ®µ

è¯¾å ‚ç»ƒä¹ ï¼š

1. æ‰“å°ç”¨æˆ·è¾“å…¥æ—¥å¿—
2. æ³¨å…¥ userId
3. æ‹¼æ¥ prompt
4. è°ƒç”¨æœ¬åœ°æ¨¡å‹
5. è§£æè¿”å›ç»“æœ

---
